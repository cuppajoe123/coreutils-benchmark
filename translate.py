# The final benchmark suite will have various C functions from the GNU coreutils manually picked out. We read these into Python from files.

# For each file/function, we generate a wrapper.h, which is a file with all the necessary #include statements. We use this file to generate Rust bindings.

# The Rust bindings will be too large of a file to feed into the LLM as context. It can be thousands of lines.
# TODO: Experiment with picking out just the definitions and declarations used in the C function

# The prompt will consist of the C code and the relevant Rust bindings, if we can trim them down enough.

# We then compile the Rust code into a shared library and we compile a modified C codebase where the translated function is declared as "extern"

# For the benchmark, the C function, wrapper.h, and bindings.rs will all be prepared ahead of time. Meanwhile, this translation script will take a customizable prompt as input, feed it to the LLM, compile, and test the result.

# Naming conventions: each test will be named after the C file being (partially) translated. So to test echo.c, run `python3 script.py prompt.txt echo`
# Each test will have a Rust project associated with it, generated by Cargo. So for echo.c, there will be a directory called echo_rust, and in it's src/ directory will be bindings.rs and lib.rs, where the translation goes.

import subprocess
import sys
import os
from openai import OpenAI

def run_build_command(command):
    try:
        # Run the cargo command
        result = subprocess.run(
            command,           # The command to run as a list
            stdout=subprocess.PIPE,   # Capture the output
            stderr=subprocess.PIPE,   # Capture errors
            text=True,                # Decode bytes to str
            check=True                # Raise exception if command fails
        )
        return result.stdout
    except subprocess.CalledProcessError as e:
        # Handle errors if the command fails
        print(f"Error occurred: {e.stderr}")
        with open("build-output/" + command[1] + ".txt", 'a', encoding='utf-8') as file:
            file.write(e.stderr)
        return "Error occurred"


# read in prompt file
with open(sys.argv[1], 'r', encoding='utf-8') as file:
    prompt = file.read()

# read in the C source code
# Example: echo_c/echo.c
with open(sys.argv[2] + "_c/" + sys.argv[2] + ".c", 'r', encoding='utf-8') as file:
    program_text = file.read()

# read in corresponding modified Rust bindings
with open(sys.argv[2] + "_rust/bindings_prompt.rs", encoding='utf-8') as file:
    bindings = file.read()

# Send to OpenAI, not the most expensive model!
client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": prompt + bindings + program_text
        }
    ]
)

rust_translation = completion.choices[0].message.content.replace("\\n", "\n")

# write translated Rust code to lib.rs in corresponding project directory
with open(sys.argv[2] + "_rust/src/lib.rs", 'a', encoding='utf-8') as file:
    file.write(rust_translation)

# attempt to build the Rust code
with open("build-output/rust-build-" + sys.argv[2] + ".txt", 'a', encoding='utf-8') as file:
    file.write(run_build_command(["cargo", "build", "--manifest-path", sys.argv[2] + "_rust/Cargo.toml", "--release"]))

# attempt to build C project using shared library compiled from Rust
# with open("build-output/c_build_" + sys.argv[2] + ".txt", 'a', encoding='utf-8') as file:
#     file.write(run_build_command(["make", "--directory=<c_directory"]))
    
# TODO: create project infrastructure and example - all the necessary directories and paths, so that this script will work.
